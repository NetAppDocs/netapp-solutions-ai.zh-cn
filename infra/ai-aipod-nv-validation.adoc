---
sidebar: sidebar 
permalink: infra/ai-aipod-nv-validation.html 
keywords: NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NVIDIA AI Enterprise, NVIDIA BasePOD, NVIDIA DGX 
summary: NetApp AIPod与NVIDIA DGX 系统 - 解决方案验证和规模调整指南 
---
= NVA-1173 NetApp AIPod与NVIDIA DGX 系统 - 解决方案验证和规模调整指南
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
本节重点介绍采用NVIDIA DGX 系统的NetApp AIPod的解决方案验证和尺寸调整指导。



== 解决方案验证

使用开源工具 FIO 通过一系列合成工作负载验证了此解决方案中的存储配置。这些测试包括读写 I/O 模式，旨在模拟执行深度学习训练作业的 DGX 系统产生的存储工作负载。使用同时运行 FIO 工作负载的 2 插槽 CPU 服务器集群来验证存储配置，以模拟 DGX 系统集群。每个客户端都配置了前面描述的相同网络配置，并添加了以下详细信息。

以下安装选项用于此验证：

[cols="30%, 70%"]
|===


| 版本=4.1 | 启用 pNFS 来并行访问多个存储节点 


| 原型=rdma | 将传输协议设置为 RDMA，而不是默认的 TCP 


| 端口=20049 | 为 RDMA NFS 服务指定正确的端口 


| 最大连接数=16 | 启用 NFS 会话中继来聚合存储端口带宽 


| 写=渴望 | 提高缓冲写入的写入性能 


| rsize=262144,wsize=262144 | 将 I/O 传输大小设置为 256k 
|===
此外，客户端的 NFS max_session_slots 值配置为 1024。由于该解决方案是使用 NFS over RDMA 进行测试的，因此存储网络端口配置了主动/被动结合。本次验证使用了以下债券参数：

[cols="30%, 70%"]
|===


| 模式=主动备份 | 将绑定设置为主动/被动模式 


| primary=<接口名称> | 所有客户端的主接口分布在交换机上 


| mii-监控间隔=100 | 指定监控间隔为100ms 


| 故障转移 mac 策略=活动 | 指定活动链路的 MAC 地址是绑定的 MAC。这是 RDMA 通过绑定接口正确运行所必需的。 
|===
存储系统配置如下，包括两个 A900 HA 对（4 个控制器），每个 HA 对连接两个 NS224 磁盘架，每个磁盘架有 24 个 1.9TB NVMe 磁盘驱动器。如架构部分所述，所有控制器的存储容量使用FlexGroup卷进行组合，并且所有客户端的数据分布在集群中的所有控制器上。



== 存储系统规模指南

NetApp已成功完成 DGX BasePOD 认证，经测试的两个 A90 HA 对可以轻松支持由 16 个 DGX H100 系统组成的集群。对于具有更高存储性能要求的大型部署，可以将额外的AFF系统添加到NetApp ONTAP集群中，单个集群中最多可包含 12 个 HA 对（24 个节点）。使用本解决方案中描述的FlexGroup技术，24 节点集群可以在单个命名空间中提供超过 79 PB 和高达 552 GBps 的吞吐量。其他NetApp存储系统（例如AFF A400、A250 和 C800）以较低的成本为较小规模的部署提供较低的性能和/或更高的容量选项。由于ONTAP 9 支持混合模型集群，客户可以从较小的初始占用空间开始，并随着容量和性能需求的增长向集群添加更多或更大的存储系统。下表粗略估计了每个AFF型号支持的 A100 和 H100 GPU 的数量。

NetApp 存储系统规模调整指南

image:aipod-nv-a90-sizing.png["该图显示输入/输出对话框或表示书面内容"]
